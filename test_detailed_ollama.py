#!/usr/bin/env python3
"""
DetaylÄ± Ollama LLM Test
"""

import sys
import os

# Python path ekle
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, "src")
pipecat_src_path = os.path.join(current_dir, "pipecat", "src")

if src_path not in sys.path:
    sys.path.insert(0, src_path)
    print(f"Added to Python path: {src_path}")

if pipecat_src_path not in sys.path:
    sys.path.insert(0, pipecat_src_path)
    print(f"Added to Python path: {pipecat_src_path}")

# soxr stub oluÅŸtur
import numpy as np
if not hasattr(sys.modules, 'soxr'):
    class SoxrStub:
        def resample(self, *args, **kwargs):
            return np.array([])
    
    sys.modules['soxr'] = SoxrStub()
    print("âœ… soxr stub module created")

import requests
import json
import time
import asyncio

# Import edebilir miyiz test edelim
try:
    from services.ollama_llm import OllamaLLMService, ConversationContext
    print("âœ… Ollama service import baÅŸarÄ±lÄ±")
except ImportError as e:
    print(f"âŒ Import hatasÄ±: {e}")
    sys.exit(1)

def test_direct_ollama_api():
    """Direct Ollama API testi"""
    
    print("\nğŸ”— Direct Ollama API Test...")
    
    url = "http://localhost:11434/api/generate"
    
    test_prompts = [
        "Merhaba, adÄ±n ne?",
        "TÃ¼rkiye'nin baÅŸkenti neresi?",
        "5 + 3 kaÃ§ eder?",
        "KÄ±sa bir ÅŸaka anlat",
        "Bu gÃ¼nÃ¼n tarihi nedir?"
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ“ Test {i}: {prompt}")
        
        payload = {
            "model": "llama3.2:3b",
            "prompt": prompt,
            "stream": False
        }
        
        start_time = time.time()
        
        try:
            response = requests.post(url, json=payload, timeout=15)
            end_time = time.time()
            
            if response.status_code == 200:
                result = response.json()
                duration = (end_time - start_time) * 1000
                
                print(f"âœ… BaÅŸarÄ±lÄ± - {duration:.0f}ms")
                print(f"ğŸ¤– YanÄ±t: {result.get('response', 'N/A')[:100]}...")
                print(f"ğŸ“Š Token sayÄ±sÄ±: {result.get('eval_count', 0)}")
                
            else:
                print(f"âŒ HTTP HatasÄ±: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ Hata: {e}")

async def test_ollama_service():
    """OllamaLLMService sÄ±nÄ±fÄ± testi"""
    
    print("\nğŸ”§ OllamaLLMService SÄ±nÄ±f Testi...")
    
    # Context artÄ±k service iÃ§inde yÃ¶netiliyor
    service = OllamaLLMService()
    
    await service.start()
    
    test_messages = [
        "Merhaba, ben bir mÃ¼ÅŸteriyim",
        "Hesap bakiyemi Ã¶ÄŸrenebilir miyim?",
        "Kredi kartÄ± baÅŸvurusu yapmak istiyorum",
        "TeÅŸekkÃ¼rler, iyi gÃ¼nler"
    ]
    
    for i, message in enumerate(test_messages, 1):
        print(f"\nğŸ’¬ Mesaj {i}: {message}")
        
        start_time = time.time()
        
        try:
            response = await service.generate_response(message)
            end_time = time.time()
            
            duration = (end_time - start_time) * 1000
            
            print(f"âœ… YanÄ±t alÄ±ndÄ± - {duration:.0f}ms")
            print(f"ğŸ¤– LLM: {response}")
            print(f"ğŸ“š Context: {len(service._context.history)} mesaj")
            
        except Exception as e:
            print(f"âŒ Hata: {e}")
    
    await service.stop()

async def test_conversation_flow():
    """KonuÅŸma akÄ±ÅŸÄ± testi"""
    
    print("\nğŸ’­ KonuÅŸma AkÄ±ÅŸÄ± Testi...")
    
    service = OllamaLLMService()
    
    await service.start()
    
    conversation = [
        ("MÃ¼ÅŸteri", "Merhaba, kredi kartÄ± baÅŸvurusu yapmak istiyorum"),
        ("MÃ¼ÅŸteri", "AylÄ±k gelirim 15.000 TL"),
        ("MÃ¼ÅŸteri", "Evet, Ã§alÄ±ÅŸma belgem var"),
        ("MÃ¼ÅŸteri", "Ne kadar sÃ¼rer onay sÃ¼reci?"),
        ("MÃ¼ÅŸteri", "TeÅŸekkÃ¼rler, baÅŸvurumu yapabilirsiniz")
    ]
    
    for i, (speaker, message) in enumerate(conversation, 1):
        print(f"\nğŸ—£ï¸ {speaker}: {message}")
        
        start_time = time.time()
        
        try:
            response = await service.generate_response(message)
            end_time = time.time()
            
            duration = (end_time - start_time) * 1000
            
            print(f"ğŸ¤– Asistan ({duration:.0f}ms): {response}")
            
            # Context'i kontrol et
            print(f"ğŸ“Š Context durumu: {len(service._context.history)} mesaj, son gÃ¼ncelleme: {service._context.last_update}")
            
        except Exception as e:
            print(f"âŒ Hata: {e}")
            break
    
    await service.stop()

if __name__ == "__main__":
    print("ğŸ§ª DetaylÄ± Ollama LLM Test Suite")
    print("=" * 50)
    
    # 1. Direct API test
    test_direct_ollama_api()
    
    # 2. Service class test
    asyncio.run(test_ollama_service())
    
    # 3. Conversation flow test
    asyncio.run(test_conversation_flow())
    
    print("\nğŸ‰ TÃ¼m testler tamamlandÄ±!") 