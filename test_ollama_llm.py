#!/usr/bin/env python3
"""
Ollama LLM entegrasyon testi
"""

import asyncio
import sys
from pathlib import Path

# Path setup
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))
sys.path.insert(0, str(project_root / "pipecat" / "src"))
sys.path.insert(0, str(project_root / "tests"))
import conftest

from services.ollama_llm import OllamaLLMService
from pipecat.frames.frames import TranscriptionFrame
import structlog

logger = structlog.get_logger()

async def test_ollama_connection():
    """Ollama baÄŸlantÄ±sÄ±nÄ± test et"""
    
    print("ğŸ”— Ollama LLM BaÄŸlantÄ± Testi...")
    
    try:
        # LLM servisini oluÅŸtur
        llm_service = OllamaLLMService()
        print("âœ… OllamaLLMService oluÅŸturuldu")
        
        # Servisi baÅŸlat
        await llm_service.start()
        print("âœ… Ollama LLM servisi baÅŸlatÄ±ldÄ±")
        
        # Servisi durdur
        await llm_service.stop()
        print("ğŸ”Œ Ollama LLM servisi durduruldu")
        
        return True
        
    except Exception as e:
        print(f"âŒ Ollama baÄŸlantÄ± hatasÄ±: {e}")
        return False

async def test_llm_generation():
    """LLM yanÄ±t Ã¼retimi testi"""
    
    print("ğŸ¤– Ollama LLM YanÄ±t Ãœretimi Testi...")
    
    try:
        llm_service = OllamaLLMService()
        await llm_service.start()
        print("âœ… LLM servisi baÅŸlatÄ±ldÄ±")
        
        # Test sorularÄ±
        test_prompts = [
            "Merhaba, nasÄ±lsÄ±n?",
            "AdÄ±n ne?",
            "Bana yardÄ±m edebilir misin?",
            "BugÃ¼n hava nasÄ±l?",
            "TeÅŸekkÃ¼r ederim"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\\nğŸ“ Test {i}: {prompt}")
            
            start_time = asyncio.get_event_loop().time()
            response = await llm_service.generate_response(prompt)
            latency_ms = (asyncio.get_event_loop().time() - start_time) * 1000
            
            print(f"ğŸ¤– YanÄ±t: {response}")
            print(f"â±ï¸ Latency: {latency_ms:.2f}ms")
            
            # Latency kontrolÃ¼ (hedef: â‰¤400ms)
            if latency_ms <= 400:
                print("âœ… Latency hedefi karÅŸÄ±landÄ±")
            else:
                print(f"âš ï¸ Latency hedefi aÅŸÄ±ldÄ± (hedef: â‰¤400ms)")
        
        await llm_service.stop()
        print("ğŸ”Œ LLM servisi durduruldu")
        
        return True
        
    except Exception as e:
        print(f"âŒ LLM yanÄ±t Ã¼retim hatasÄ±: {e}")
        return False

async def test_conversation_context():
    """KonuÅŸma geÃ§miÅŸi testi"""
    
    print("ğŸ’¬ KonuÅŸma GeÃ§miÅŸi Testi...")
    
    try:
        llm_service = OllamaLLMService()
        await llm_service.start()
        print("âœ… LLM servisi baÅŸlatÄ±ldÄ±")
        
        # KonuÅŸma senaryosu
        conversation = [
            "Merhaba, adÄ±m Ali.",
            "BugÃ¼n nasÄ±l hissediyorsun?",
            "Benim adÄ±mÄ± hatÄ±rlÄ±yor musun?",
            "TeÅŸekkÃ¼r ederim, gÃ¼le gÃ¼le!"
        ]
        
        for i, user_input in enumerate(conversation, 1):
            print(f"\\nğŸ‘¤ KullanÄ±cÄ± {i}: {user_input}")
            
            response = await llm_service.generate_response(user_input)
            print(f"ğŸ¤– Asistan {i}: {response}")
        
        # Context'i temizle
        llm_service.clear_context()
        print("\\nğŸ§¹ KonuÅŸma geÃ§miÅŸi temizlendi")
        
        await llm_service.stop()
        print("ğŸ”Œ LLM servisi durduruldu")
        
        return True
        
    except Exception as e:
        print(f"âŒ KonuÅŸma geÃ§miÅŸi test hatasÄ±: {e}")
        return False

async def test_llm_frames():
    """LLM frame Ã¼retimi testi"""
    
    print("ğŸ“¦ LLM Frame Ãœretimi Testi...")
    
    try:
        llm_service = OllamaLLMService()
        await llm_service.start()
        print("âœ… LLM servisi baÅŸlatÄ±ldÄ±")
        
        # Test metni
        test_text = "Merhaba, size nasÄ±l yardÄ±mcÄ± olabilirim?"
        print(f"ğŸ“ Test metni: {test_text}")
        
        # Frame'leri Ã¼ret
        frames = await llm_service.run_llm(test_text)
        print(f"ğŸ“¦ Ãœretilen frame sayÄ±sÄ±: {len(frames)}")
        
        for i, frame in enumerate(frames):
            print(f"Frame {i+1}: {type(frame).__name__}")
            if hasattr(frame, 'text'):
                print(f"  Metin: {frame.text[:100]}...")
        
        await llm_service.stop()
        print("ğŸ”Œ LLM servisi durduruldu")
        
        return True
        
    except Exception as e:
        print(f"âŒ LLM frame test hatasÄ±: {e}")
        return False

async def test_error_handling():
    """Hata yÃ¶netimi testi"""
    
    print("ğŸš¨ Hata YÃ¶netimi Testi...")
    
    try:
        # YanlÄ±ÅŸ URL ile servis oluÅŸtur
        llm_service = OllamaLLMService(url="http://localhost:99999/api/generate")
        
        try:
            await llm_service.start()
            print("âŒ HatalÄ± URL ile baÄŸlantÄ± baÅŸarÄ±lÄ± olmamalÄ±ydÄ±")
            return False
        except Exception as e:
            print(f"âœ… Beklenen hata yakalandÄ±: {e}")
        
        # DoÄŸru servis ile timeout testi
        llm_service = OllamaLLMService(timeout=0.001)  # Ã‡ok kÄ±sa timeout
        await llm_service.start()
        
        response = await llm_service.generate_response("Test")
        print(f"ğŸ¤– Timeout yanÄ±tÄ±: {response}")
        
        await llm_service.stop()
        
        return True
        
    except Exception as e:
        print(f"âŒ Hata yÃ¶netimi test hatasÄ±: {e}")
        return False

async def main():
    """Ana test fonksiyonu"""
    
    print("ğŸš€ Ollama LLM Integration Test Suite")
    print("=" * 60)
    
    tests = [
        ("BaÄŸlantÄ± Testi", test_ollama_connection),
        ("YanÄ±t Ãœretimi", test_llm_generation),
        ("KonuÅŸma GeÃ§miÅŸi", test_conversation_context),
        ("Frame Ãœretimi", test_llm_frames),
        ("Hata YÃ¶netimi", test_error_handling),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\\n{len(results)+1}ï¸âƒ£ {test_name}:")
        print("-" * 50)
        
        try:
            result = await test_func()
            results.append((test_name, result))
            
        except Exception as e:
            print(f"âŒ Test hatasÄ±: {e}")
            results.append((test_name, False))
    
    # SonuÃ§larÄ± Ã¶zetle
    print("\\n" + "=" * 60)
    print("ğŸ“Š Test SonuÃ§larÄ±:")
    
    passed = 0
    for test_name, result in results:
        status = "âœ…" if result else "âŒ"
        print(f"  {status} {test_name}")
        if result:
            passed += 1
    
    print(f"\\nğŸ¯ BaÅŸarÄ± OranÄ±: {passed}/{len(results)} ({passed/len(results)*100:.1f}%)")
    
    if passed == len(results):
        print("ğŸ‰ TÃ¼m LLM testleri baÅŸarÄ±lÄ±! Ollama entegrasyonu hazÄ±r!")
    else:
        print("âš ï¸ BazÄ± testler baÅŸarÄ±sÄ±z. Ollama server'Ä±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.")
    
    print("\\nâœ¨ Ollama LLM test suite tamamlandÄ±!")

if __name__ == "__main__":
    asyncio.run(main()) 