syntax = "proto3";

package opensips.ai.llm;

import "google/protobuf/empty.proto";

option go_package = "github.com/opensips/ai-voice-connector/proto/llm";

// Simple LLM service - based on legacy WebSocket implementation
service LLMService {
  // Process text and generate streaming response (main method)
  rpc ProcessText(TextProcessingRequest) returns (stream TextResponse);
  
  // Update context
  rpc UpdateContext(ContextUpdateRequest) returns (ContextResponse);
  
  // Health check
  rpc HealthCheck(google.protobuf.Empty) returns (HealthResponse);
  
  // Get service stats
  rpc GetStats(google.protobuf.Empty) returns (StatsResponse);
}

// Text processing request (matching legacy WebSocket JSON format)
message TextProcessingRequest {
  string text = 1;                    // User prompt (was "prompt" in legacy)
  string system_prompt = 2;           // System prompt (optional)
  
  // Generation parameters (same as legacy WebSocket params)
  float temperature = 3;              // 0.0 - 2.0 (default 0.2)
  int32 max_tokens = 4;               // Max response tokens (default 80)
  float top_p = 5;                    // Top-p sampling (default 0.9)
  repeated string stop = 6;           // Stop sequences
}

// Streaming text response (matching legacy WebSocket response pattern)
message TextResponse {
  string chunk = 1;                   // Text chunk
  bool done = 2;                      // True when generation complete
}

// Context update request
message ContextUpdateRequest {
  string session_id = 1;
  map<string, string> updates = 2;
}

// Context response
message ContextResponse {
  bool success = 1;
  string message = 2;
}

// Health check response
message HealthResponse {
  enum Status {
    UNKNOWN = 0;
    SERVING = 1;
    NOT_SERVING = 2;
  }
  Status status = 1;
  string message = 2;
  string model_loaded = 3;
}

// Service statistics
message StatsResponse {
  int32 total_requests = 1;
  int64 uptime_seconds = 2;
  string model_info = 3;
}