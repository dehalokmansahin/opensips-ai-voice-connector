#!/usr/bin/env python3
"""
LLM Streaming Performance Test
"""

import sys
import os
import time
import asyncio

# Python path ekle
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, "src")
pipecat_src_path = os.path.join(current_dir, "pipecat", "src")

if src_path not in sys.path:
    sys.path.insert(0, src_path)
if pipecat_src_path not in sys.path:
    sys.path.insert(0, pipecat_src_path)

# soxr stub
import numpy as np
if not hasattr(sys.modules, 'soxr'):
    class SoxrStub:
        def resample(self, *args, **kwargs):
            return np.array([])
    sys.modules['soxr'] = SoxrStub()

from services.ollama_llm import OllamaLLMService

async def test_streaming_vs_regular():
    """Streaming vs Regular LLM yanÄ±t sÃ¼resi karÅŸÄ±laÅŸtÄ±rmasÄ±"""
    
    print("ğŸš€ LLM Streaming Performance Test")
    print("=" * 50)
    
    service = OllamaLLMService()
    await service.start()
    
    test_messages = [
        "Merhaba, nasÄ±lsÄ±nÄ±z?",
        "Kredi kartÄ± baÅŸvurusu yapmak istiyorum",
        "Hesap bakiyemi Ã¶ÄŸrenebilir miyim?",
        "Bu gÃ¼nÃ¼n tarihi nedir?",
        "TeÅŸekkÃ¼rler, iyi gÃ¼nler"
    ]
    
    for i, message in enumerate(test_messages, 1):
        print(f"\nğŸ“ Test {i}: {message}")
        
        # 1. Regular (non-streaming) test
        print("ğŸ”„ Regular LLM:")
        start_time = time.time()
        
        try:
            response = await service.generate_response(message)
            end_time = time.time()
            regular_duration = (end_time - start_time) * 1000
            
            print(f"  â±ï¸ Total: {regular_duration:.0f}ms")
            print(f"  ğŸ“ Response: {response[:80]}...")
            
        except Exception as e:
            print(f"  âŒ Error: {e}")
            regular_duration = 0
        
        # 2. Streaming test
        print("âš¡ Streaming LLM:")
        start_time = time.time()
        first_token_time = None
        token_count = 0
        full_response = ""
        
        try:
            async for token in service.generate_response_streaming(message):
                if first_token_time is None:
                    first_token_time = time.time()
                    first_token_latency = (first_token_time - start_time) * 1000
                    print(f"  ğŸ¯ First token: {first_token_latency:.0f}ms")
                
                token_count += 1
                full_response += token
            
            end_time = time.time()
            total_duration = (end_time - start_time) * 1000
            
            print(f"  â±ï¸ Total: {total_duration:.0f}ms")
            print(f"  ğŸ”¢ Tokens: {token_count}")
            print(f"  ğŸ“ Response: {full_response[:80]}...")
            
            # Performance comparison
            if regular_duration > 0:
                improvement = ((regular_duration - (first_token_time - start_time) * 1000) / regular_duration) * 100
                print(f"  ğŸ“Š First token improvement: {improvement:.1f}%")
                
        except Exception as e:
            print(f"  âŒ Error: {e}")
    
    await service.stop()

async def test_streaming_pipeline():
    """Pipeline streaming test"""
    
    print("\nğŸ”— Pipeline Streaming Test")
    print("=" * 30)
    
    from pipeline.stages import LLMProcessor
    from pipecat.frames.frames import TranscriptionFrame, StartFrame, EndFrame
    
    # Mock pipeline iÃ§in basit frame collector
    class FrameCollector:
        def __init__(self):
            self.frames = []
            self.first_llm_frame_time = None
            self.start_time = None
        
        async def process_frame(self, frame, direction=None):
            if self.start_time is None:
                self.start_time = time.time()
            
            self.frames.append(frame)
            
            # Ä°lk LLM text frame timing
            if hasattr(frame, 'text') and frame.text and self.first_llm_frame_time is None:
                self.first_llm_frame_time = time.time()
                latency = (self.first_llm_frame_time - self.start_time) * 1000
                print(f"  ğŸ¯ First LLM token: {latency:.0f}ms")
                print(f"  ğŸ“ Token: '{frame.text}'")
    
    # LLM processor test
    llm_processor = LLMProcessor()
    frame_collector = FrameCollector()
    llm_processor._next_processor = frame_collector
    
    # Start the processor
    await llm_processor.process_frame(StartFrame())
    
    # Test transcription
    test_text = "Merhaba, kredi kartÄ± baÅŸvurusu yapmak istiyorum"
    print(f"ğŸ“ Input: {test_text}")
    
    transcription_frame = TranscriptionFrame(text=test_text, user_id="test", timestamp=time.time())
    
    start_time = time.time()
    await llm_processor.process_frame(transcription_frame)
    end_time = time.time()
    
    total_duration = (end_time - start_time) * 1000
    print(f"â±ï¸ Total pipeline: {total_duration:.0f}ms")
    print(f"ğŸ”¢ Total frames: {len(frame_collector.frames)}")
    
    # Stop the processor
    await llm_processor.process_frame(EndFrame())

async def test_real_time_simulation():
    """GerÃ§ek zamanlÄ± konuÅŸma simÃ¼lasyonu"""
    
    print("\nğŸ™ï¸ Real-time Conversation Simulation")
    print("=" * 40)
    
    service = OllamaLLMService()
    await service.start()
    
    conversation = [
        "Merhaba",
        "Kredi kartÄ± baÅŸvurusu yapmak istiyorum",
        "AylÄ±k gelirim 15 bin lira",
        "Evet, Ã§alÄ±ÅŸma belgem var",
        "TeÅŸekkÃ¼rler"
    ]
    
    total_conversation_time = 0
    
    for i, user_input in enumerate(conversation, 1):
        print(f"\nğŸ‘¤ KullanÄ±cÄ±: {user_input}")
        
        start_time = time.time()
        first_token_received = False
        response_text = ""
        
        print("ğŸ¤– Asistan: ", end="", flush=True)
        
        async for token in service.generate_response_streaming(user_input):
            if not first_token_received:
                first_token_time = time.time()
                first_token_latency = (first_token_time - start_time) * 1000
                print(f"[{first_token_latency:.0f}ms] ", end="", flush=True)
                first_token_received = True
            
            print(token, end="", flush=True)
            response_text += token
            
            # Simulate real-time typing (optional)
            await asyncio.sleep(0.01)  # 10ms per token
        
        end_time = time.time()
        total_time = (end_time - start_time) * 1000
        total_conversation_time += total_time
        
        print(f" [{total_time:.0f}ms total]")
    
    print(f"\nğŸ“Š Conversation Summary:")
    print(f"  Total time: {total_conversation_time:.0f}ms")
    print(f"  Average per exchange: {total_conversation_time/len(conversation):.0f}ms")
    
    await service.stop()

if __name__ == "__main__":
    print("ğŸ§ª LLM Streaming Performance Test Suite")
    print("=" * 50)
    
    asyncio.run(test_streaming_vs_regular())
    asyncio.run(test_streaming_pipeline())
    asyncio.run(test_real_time_simulation())
    
    print("\nğŸ‰ Streaming tests completed!")
    print("\nğŸ“ˆ Expected improvements:")
    print("  â€¢ First token latency: 400-800ms (vs 2000+ms)")
    print("  â€¢ User experience: Immediate response start")
    print("  â€¢ Pipeline throughput: Higher concurrent capacity") 